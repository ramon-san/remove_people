{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62066efe",
   "metadata": {},
   "source": [
    "# Remove People\n",
    "\n",
    "This code is a proof of concept of a program that removes people or unwanted objects from an image. Before running follow the installation guide lines specified in the `README.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torchvision\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API\"))\n",
    "\n",
    "# For best results use images that have a 1:1 aspect ratio. \n",
    "# This limitation is given by the OpenAI API.\n",
    "# If you change this provider to one that accept any aspect ration, ignore this.\n",
    "image_name = 'example_2.jpg' # Only edit this line to change the image.\n",
    "\n",
    "# Add known path to the image.\n",
    "image_path = f\"images/original/{image_name}\"\n",
    "# Clean name of image without the extension.\n",
    "clean_image_name = image_name.split('.')[0]\n",
    "\n",
    "# Create image folders if they don't exist.\n",
    "os.makedirs(\"images/original\", exist_ok=True)\n",
    "os.makedirs(\"images/crops\", exist_ok=True)\n",
    "os.makedirs(\"images/results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b91402",
   "metadata": {},
   "source": [
    "## Load Mask R-CNN\n",
    "\n",
    "For this we use a [Mask R-CNN](https://arxiv.org/abs/1703.06870) model pre-trained on the [COCO dataset](https://cocodataset.org/). The model is loaded from [`pytorch`](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html) using default weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab883eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Mask R-CNN model.\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "# Set the model to evaluation mode.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9926e94",
   "metadata": {},
   "source": [
    "## Miscellaneous Functions\n",
    "\n",
    "Utility functions to pre-process the image and apply masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocess the input image for segmentation.\n",
    "\n",
    "    This function opens an image from the given path, converts it to RGB, \n",
    "    and transforms it into a tensor suitable for input into a neural network.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image file.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The preprocessed image tensor with shape (1, C, H, W).\n",
    "        PIL.Image.Image: The original image.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0), image\n",
    "\n",
    "def apply_mask(image, mask):\n",
    "    \"\"\"\n",
    "    Post-process the segmentation mask and apply it to the input image.\n",
    "\n",
    "    This function resizes the mask to match the input image dimensions,\n",
    "    applies a threshold to create a binary mask, and sets the unwanted \n",
    "    object pixels to black.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): The original image.\n",
    "        mask (torch.Tensor): The segmentation mask with shape (1, H, W).\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: The image with the mask applied, with unwanted object pixels set to black.\n",
    "    \"\"\"\n",
    "    image = np.array(image)\n",
    "    mask = mask.squeeze().cpu().numpy()\n",
    "    mask = mask > 0.5  # Apply threshold to create binary mask\n",
    "    mask = np.resize(mask, (image.shape[0], image.shape[1]))  # Resize mask to match image dimensions\n",
    "    image[mask] = [0, 0, 0]  # Set unwanted object pixels to black (you can change this to any processing)\n",
    "    return Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64867b",
   "metadata": {},
   "source": [
    "## Apply Model and Masks\n",
    "\n",
    "Main function that applies the model to the image and returns the masked image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea030a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input image\n",
    "input_tensor, original_image = preprocess_image(image_path)\n",
    "\n",
    "# Perform segmentation\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    predictions = model(input_tensor)\n",
    "\n",
    "# Extract masks, labels, and scores from the model's predictions\n",
    "masks = predictions[0]['masks']\n",
    "labels = predictions[0]['labels']\n",
    "scores = predictions[0]['scores']\n",
    "\n",
    "# Convert the original image to a numpy array for processing\n",
    "input_image_np = np.array(original_image)\n",
    "input_image_masked = np.array(original_image)\n",
    "\n",
    "# Apply each mask to the original image\n",
    "for mask in masks:\n",
    "    input_image_masked = apply_mask(input_image_masked, mask)\n",
    "\n",
    "# Plot the original image and the object detection masks.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(input_image_np)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(input_image_masked)\n",
    "axes[1].set_title('Object Detection Masks')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbe7e08",
   "metadata": {},
   "source": [
    "## Inspect Object Detection\n",
    "\n",
    "Function to visualize the object detection results, note-down the indices of the main objects (objects might appear more than once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each mask individually\n",
    "num_masks = len(masks)\n",
    "cols = 5  # Number of columns in the plot grid\n",
    "rows = (num_masks // cols) + (num_masks % cols > 0)  # Calculate the number of rows\n",
    "\n",
    "# Parse CSV file with the labels for the COCO dataset.\n",
    "coco_labels = []\n",
    "with open('coco_labels.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        coco_labels.append(line.strip())\n",
    "\n",
    "plt.figure(figsize=(20, 5 * rows))\n",
    "for i, mask in enumerate(masks):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    mask_np = mask.squeeze().cpu().numpy()\n",
    "    mask_resized = np.array(Image.fromarray(mask_np).resize((original_image.width, original_image.height)))\n",
    "    plt.imshow(mask_resized, cmap='gray')\n",
    "    plt.title(f'{i} - {coco_labels[labels[i].item()]} ({scores[i]:.2f})')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1055581",
   "metadata": {},
   "source": [
    "## Protagonist Selection\n",
    "\n",
    "Input the indices that correspond to the main objects in the image and remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the indices of the main protagonists in the image, check the labels above.\n",
    "main_protagonist_indices = [0, 1, 4, 22, 59]\n",
    "\n",
    "# Filter out masks of main protagonists.\n",
    "protagonist_masks = [masks[i] for i in main_protagonist_indices]\n",
    "\n",
    "# Create a combined mask for the main protagonists.\n",
    "combined_mask_protagonists = np.zeros(input_image_np.shape[:2], dtype=np.uint8)\n",
    "for mask in protagonist_masks:\n",
    "    combined_mask_protagonists = np.maximum(combined_mask_protagonists, mask.squeeze().cpu().numpy())\n",
    "\n",
    "# Create a combined mask for all objects.\n",
    "combined_mask_all = np.zeros(input_image_np.shape[:2], dtype=np.uint8)\n",
    "for mask in masks:\n",
    "    combined_mask_all = np.maximum(combined_mask_all, mask.squeeze().cpu().numpy())\n",
    "\n",
    "# Subtract the protagonist mask from the combined mask to get the non-protagonist mask.\n",
    "non_protagonist_mask = combined_mask_all - combined_mask_protagonists\n",
    "non_protagonist_mask = (non_protagonist_mask > 0).astype(np.uint8)  # Ensure it's binary.\n",
    "\n",
    "# Plot the masks side by side.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(combined_mask_protagonists, cmap='gray')\n",
    "axes[0].set_title('Protagonist Mask')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(non_protagonist_mask, cmap='gray')\n",
    "axes[1].set_title('Combined Mask of All Objects')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926c5c5",
   "metadata": {},
   "source": [
    "## PNG Creation\n",
    "\n",
    "Create a PNG file of the protagonist without unwanted objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the mask to keep the protagonists and background\n",
    "final_mask = 1 - non_protagonist_mask\n",
    "\n",
    "# Apply the final mask to the image\n",
    "masked_image = input_image_np.copy()\n",
    "masked_image[final_mask == 0] = 0  # Set non-used areas to 0 (black)\n",
    "\n",
    "# Create an alpha channel where 0 is transparent and 255 is opaque\n",
    "alpha_channel = np.where(final_mask == 1, 255, 0).astype(np.uint8)\n",
    "\n",
    "# Convert to PIL Image and save as PNG with transparent background\n",
    "result_image = Image.fromarray(masked_image.astype('uint8'), 'RGB')\n",
    "result_image.putalpha(Image.fromarray(alpha_channel))\n",
    "result_image.save(f'images/crops/{clean_image_name}.png', format='PNG')\n",
    "\n",
    "# Display the resulting image\n",
    "plt.imshow(result_image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873b41f",
   "metadata": {},
   "source": [
    "## OpenAI Pre-processing\n",
    "\n",
    "The [OpenAI API being used](https://platform.openai.com/docs/api-reference/images/createEdit) to generate content requires the image to be of 1024x1024 pixels and smaller than 4MB. This function resizes the image to the required dimensions and saves it as a PNG file. If for any reason the image is still larger than 4MB, please compress it further and add it to the `images/crops` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf41bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the result image to be a 1024x1024 square if needed.\n",
    "def crop_image(image_path, size=(1024, 1024)):\n",
    "    image = Image.open(image_path)\n",
    "    image.thumbnail(size)\n",
    "    image.save(f'images/crops/{clean_image_name}.png', format='PNG')\n",
    "\n",
    "crop_image(f'images/crops/{clean_image_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5541f99",
   "metadata": {},
   "source": [
    "## OpenAI Execution and Download\n",
    "\n",
    "Call the OpenAI API for content generation and download the result to the `images/results` folder. If you are not happy with the result you can re-run the code, the API will always generate different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10549f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the OpenAI API to complete the image.\n",
    "response = client.images.edit(\n",
    "  model=\"dall-e-2\", # Only DALL-E 2 was supported at the time of writing.\n",
    "  image=open(f\"images/crops/{clean_image_name}.png\", \"rb\"),\n",
    "  prompt=\"A family photo in Petra that needs completion.\",\n",
    "  n=1, # If you want more results, you can increase this number.\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response.data[0].url\n",
    "\n",
    "# Download image from the URL and save in images/results folder.\n",
    "image_data = requests.get(image_url).content\n",
    "with open(f'images/results/{clean_image_name}.jpg', 'wb') as image_file:\n",
    "    image_file.write(image_data)\n",
    "\n",
    "# Display the final image.\n",
    "final_image = Image.open(f'images/results/{clean_image_name}.jpg')\n",
    "\n",
    "# Plot the original image and the result image.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(input_image_np)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(final_image)\n",
    "axes[1].set_title('Result Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71031bdb",
   "metadata": {},
   "source": [
    "## Additional Information\n",
    "\n",
    "The following cell just gets the labels being used by our model's weights. The result of this cell can be found in the [`coco_labels.txt`](coco_labels.txt) file. Run this in case you try a different set of weights, save the copied results into the [`coco_labels.txt`](coco_labels.txt) file. This project uses `MaskRCNN_ResNet50_FPN_Weights.DEFAULT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b02c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "weights_val = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "categories_val = weights_val.meta[\"categories\"]\n",
    "\n",
    "# Save the labels with index in CSV format.\n",
    "result = \"\"\n",
    "for i, category in enumerate(categories_val):\n",
    "  result += f\"{category}\\n\"\n",
    "\n",
    "# Copy the result to clipboard.\n",
    "os.system(f\"echo '{result}' | pbcopy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "removepeople",
   "language": "python",
   "name": "removepeople"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
